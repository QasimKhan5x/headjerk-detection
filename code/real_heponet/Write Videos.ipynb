{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9a5448",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\heponet\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\heponet\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\heponet\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\heponet\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Documents\\ComputerScience\\Hackathons\\SAAI\\headjerk-detection\\code\\keras_layers\\keras_layer_DecodeDetections.py:174: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\heponet\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\heponet\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\heponet\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\heponet\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from math import sin, radians\n",
    "\n",
    "from keras_ssd512 import ssd_512\n",
    "\n",
    "from architectures import mpatacchiola_generic\n",
    "from head_detector_utils import get_head_bboxes, get_cropped_pics\n",
    "from pose_estimator_utils import get_pose\n",
    "\n",
    "# Paths\n",
    "\n",
    "detector_file = 'head-detector.h5'\n",
    "estimator_file = 'pose-estimator.h5'\n",
    "\n",
    "models_path = 'models/'\n",
    "\n",
    "detector_path = models_path + detector_file\n",
    "estimator_path = models_path + estimator_file\n",
    "\n",
    "# Detector parameters.\n",
    "\n",
    "in_size_detector = 512\n",
    "confidence_threshold = 0.2\n",
    "\n",
    "# Estimator parameters.\n",
    "\n",
    "in_size_estimator = 64\n",
    "num_conv_blocks = 6\n",
    "num_filters_start = 32\n",
    "num_dense_layers = 1\n",
    "dense_layer_size = 512\n",
    "\n",
    "# Normalization parameters.\n",
    "\n",
    "mean = 0.408808\n",
    "std = 0.237583\n",
    "\n",
    "t_mean = -0.041212\n",
    "t_std = 0.323931\n",
    "\n",
    "p_mean = -0.000276\n",
    "p_std = 0.540958\n",
    "\n",
    "# Models.\n",
    "\n",
    "head_detector = ssd_512(image_size=(in_size_detector, in_size_detector, 3), n_classes=1, min_scale=0.1, max_scale=1, mode='inference')\n",
    "head_detector.load_weights(detector_path)\n",
    "\n",
    "pose_estimator = mpatacchiola_generic(in_size_estimator, num_conv_blocks, num_filters_start, num_dense_layers, dense_layer_size)\n",
    "pose_estimator.load_weights(estimator_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74da959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(example_path, num_people, result_path=None):\n",
    "    tilts = [list() for i in range(num_people)]\n",
    "    pans = [list() for i in range(num_people)]\n",
    "\n",
    "    cap = cv2.VideoCapture(example_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    if result_path:\n",
    "        out = cv2.VideoWriter(result_path, cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                              cap.get(cv2.CAP_PROP_FPS), (frame_width, frame_height))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, img = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print('Could not capture a valid frame from video source, check your cam/video value...')\n",
    "            break\n",
    "\n",
    "        # Get bounding boxes for every detected head in the picture.\n",
    "        bboxes = get_head_bboxes(img, head_detector, confidence_threshold)\n",
    "\n",
    "        # Get cropped pics for every valid bounding box.\n",
    "        gray_pic = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        heads = get_cropped_pics(gray_pic, bboxes, in_size_estimator, 0, cropping='small')\n",
    "        # For each cropped picture:\n",
    "        for i in range(len(heads)):\n",
    "            \n",
    "\n",
    "            # If it is a valid picture:\n",
    "            if heads[i].shape == (in_size_estimator, in_size_estimator):\n",
    "\n",
    "                # Get pose values.\n",
    "                tilt, pan = get_pose(heads[i], pose_estimator, img_norm = [mean, std], tilt_norm = [t_mean, t_std],\n",
    "                                     pan_norm = [p_mean, p_std], rescale=90.0)\n",
    "\n",
    "                tilts[i].append(tilt)\n",
    "                pans[i].append(pan)\n",
    "                    \n",
    "                \n",
    "                 # Get minimum and maximum values for both axes of the bounding box.\n",
    "                xmin, ymin, xmax, ymax = bboxes[i]\n",
    "\n",
    "                # Draw detection in the original picture..\n",
    "\n",
    "                rect = cv2.rectangle(img, (xmax, ymin), (xmin, ymax), (0, 255, 0), 2, lineType=cv2.LINE_AA)\n",
    "                cv2.putText(rect, 'TILT: ' + str(round(tilt, 2)) + ' PAN: ' + str(round(pan, 2)), (xmin, ymin - 10), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "                # Draw arrow from the center of the picture in the direction of the pose in the original picture.\n",
    "\n",
    "                centerx = int((xmin + xmax) / 2)\n",
    "                centery = int((ymin + ymax) / 2)\n",
    "                center = (centerx, centery)\n",
    "\n",
    "                max_arrow_len = (xmax - xmin + 1) / 2\n",
    "\n",
    "                offset_x = -1 * int(sin(radians(pan)) * max_arrow_len)\n",
    "                offset_y = -1 * int(sin(radians(tilt)) * max_arrow_len)\n",
    "\n",
    "                end = (centerx + offset_x, centery + offset_y)\n",
    "                cv2.arrowedLine(img, center, end, (0, 0, 255), 2, line_type=cv2.LINE_AA)\n",
    "                \n",
    "        if result_path:\n",
    "            out.write(img)\n",
    "        \n",
    "    cap.release()\n",
    "    if result_path:\n",
    "        out.release()\n",
    "        \n",
    "    return np.array(tilts), np.array(pans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa26db6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[280, 171, 471, 453], [907, 281, 995, 418]]\n"
     ]
    }
   ],
   "source": [
    "examples_path = os.path.join(\"..\", \"test_videos\", \"examples\")\n",
    "results_path = os.path.join(\"..\", \"test_videos\", \"results\")\n",
    "results_arrays_path = os.path.join(\"..\", \"test_videos\", \"results_arrays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70bd8230",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[868, 178, 1096, 467], [296, 285, 390, 426]]\n",
      "[[859, 184, 1089, 465], [286, 305, 366, 431]]\n",
      "[[869, 193, 1089, 462], [299, 298, 374, 434]]\n",
      "[[885, 187, 1106, 457], [295, 294, 376, 419]]\n",
      "[[837, 191, 1059, 458], [302, 305, 379, 432]]\n",
      "[[861, 180, 1091, 455], [297, 301, 376, 427]]\n",
      "[[862, 192, 1089, 459], [297, 305, 379, 439]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 8):\n",
    "    path = os.path.join(examples_path, \"1\", f\"{i}.mp4\")\n",
    "    get_angles(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec9981e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[280, 171, 471, 453], [907, 281, 995, 418]]\n",
      "[[296, 165, 483, 435], [914, 280, 1001, 411]]\n",
      "[[321, 174, 492, 408], [918, 280, 1007, 413]]\n",
      "[[315, 165, 502, 436], [903, 284, 993, 407]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    path = os.path.join(examples_path, \"2\", f\"{i}.mp4\")\n",
    "    get_angles(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74794f48",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "..\\test_videos\\examples\\1\\1.mp4\n",
      "..\\test_videos\\results\\1\\1.mp4\n",
      "..\\test_videos\\results_arrays\\1\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\1\\2.mp4\n",
      "..\\test_videos\\results\\1\\2.mp4\n",
      "..\\test_videos\\results_arrays\\1\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\1\\3.mp4\n",
      "..\\test_videos\\results\\1\\3.mp4\n",
      "..\\test_videos\\results_arrays\\1\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\1\\4.mp4\n",
      "..\\test_videos\\results\\1\\4.mp4\n",
      "..\\test_videos\\results_arrays\\1\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\1\\5.mp4\n",
      "..\\test_videos\\results\\1\\5.mp4\n",
      "..\\test_videos\\results_arrays\\1\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\1\\6.mp4\n",
      "..\\test_videos\\results\\1\\6.mp4\n",
      "..\\test_videos\\results_arrays\\1\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\1\\7.mp4\n",
      "..\\test_videos\\results\\1\\7.mp4\n",
      "..\\test_videos\\results_arrays\\1\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\2\\1.mp4\n",
      "..\\test_videos\\results\\2\\1.mp4\n",
      "..\\test_videos\\results_arrays\\2\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\2\\2.mp4\n",
      "..\\test_videos\\results\\2\\2.mp4\n",
      "..\\test_videos\\results_arrays\\2\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\2\\3.mp4\n",
      "..\\test_videos\\results\\2\\3.mp4\n",
      "..\\test_videos\\results_arrays\\2\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\2\\4.mp4\n",
      "..\\test_videos\\results\\2\\4.mp4\n",
      "..\\test_videos\\results_arrays\\2\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "=======================================================\n",
      "..\\test_videos\\examples\\2\\5.mp4\n",
      "..\\test_videos\\results\\2\\5.mp4\n",
      "..\\test_videos\\results_arrays\\2\n",
      "=======================================================\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n"
     ]
    }
   ],
   "source": [
    "for (root,dirs,files) in os.walk(examples_path, topdown=True):\n",
    "    if root[-1] not in ('1', '2'):    \n",
    "        continue\n",
    "    for file in files:\n",
    "        video_path = os.path.join(root, file)\n",
    "        folder = video_path.split('\\\\')[-2]\n",
    "        file = video_path.split('\\\\')[-1][:-4]\n",
    "        result_path = os.path.join(results_path, folder, file + \".mp4\")\n",
    "        results_array_path = os.path.join(results_arrays_path, folder)\n",
    "        print('=======================================================')\n",
    "        print(video_path)\n",
    "        print(result_path)\n",
    "        print(results_array_path)\n",
    "        print('=======================================================')\n",
    "        (tilts_1, pans_1), (tilts_2, pans_2) = get_angles(video_path, result_path)\n",
    "        \n",
    "        np.save(f\"{results_array_path}\\\\{file}_tilts_1.npy\", tilts_1)\n",
    "        np.save(f\"{results_array_path}\\\\{file}_tilts_2.npy\", tilts_2)\n",
    "        np.save(f\"{results_array_path}\\\\{file}_pans_1.npy\", pans_1)\n",
    "        np.save(f\"{results_array_path}\\\\{file}_pans_2.npy\", pans_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ca377",
   "metadata": {},
   "source": [
    "## Akshita Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dd9bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not capture a valid frame from video source, check your cam/video value...\n",
      "Could not capture a valid frame from video source, check your cam/video value...\n"
     ]
    }
   ],
   "source": [
    "video1 = os.path.join('..', 'test_videos', 'Test1.mp4')\n",
    "video2 = os.path.join('..', 'test_videos', 'Test2.mp4')\n",
    "\n",
    "res_video1 = os.path.join('..', 'test_videos', 'results', 'Test1.mp4')\n",
    "res_video2 = os.path.join('..', 'test_videos', 'results', 'Test2.mp4')\n",
    "\n",
    "res_arrays_video1 = os.path.join('..', 'test_videos', 'results_arrays', 'Test1')\n",
    "res_arrays_video2 = os.path.join('..', 'test_videos', 'results_arrays', 'Test2')\n",
    "\n",
    "tilts, pans = get_angles(video1, 1, res_video1)\n",
    "np.save(f\"{res_arrays_video1}_tilts.npy\", tilts)\n",
    "np.save(f\"{res_arrays_video1}_pans.npy\", pans)\n",
    "\n",
    "tilts, pans = get_angles(video2, 1, res_video2)\n",
    "np.save(f\"{res_arrays_video2}_tilts.npy\", tilts)\n",
    "np.save(f\"{res_arrays_video2}_pans.npy\", pans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47778f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
